{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aegis-AI Audio Deepfake Detection Training\n",
        "\n",
        "This notebook trains a lightweight CNN for audio deepfake detection using the ASVspoof 2019 dataset.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Upload this notebook to Google Colab\n",
        "2. Enable GPU: Runtime → Change runtime type → GPU (T4 or better)\n",
        "3. Run cells in order\n",
        "4. Download the trained ONNX model at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement onnxruntime (from versions: none)\n",
            "ERROR: No matching distribution found for onnxruntime\n"
          ]
        }
      ],
      "source": [
        "!pip install -q onnx onnxruntime soundfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository (or upload files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Directories created\n"
          ]
        }
      ],
      "source": [
        "# Option A: Clone from GitHub (if your repo is public)\n",
        "# !git clone https://github.com/your-username/aegis-ai.git\n",
        "# %cd aegis-ai\n",
        "\n",
        "# Option B: Upload files manually\n",
        "# 1. Upload ml/training/train_audio.py\n",
        "# 2. Upload ml/training/logging_config.py\n",
        "# 3. Upload ml/datasets/loader.py\n",
        "# 4. Upload manifest file (or download dataset)\n",
        "\n",
        "import os\n",
        "os.makedirs('ml/training', exist_ok=True)\n",
        "os.makedirs('ml/datasets', exist_ok=True)\n",
        "os.makedirs('models/audio', exist_ok=True)\n",
        "\n",
        "print(\"✓ Directories created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Training Script and Dependencies\n",
        "\n",
        "Use the file upload widget to upload:\n",
        "- `ml/training/train_audio.py`\n",
        "- `ml/training/logging_config.py`\n",
        "- `ml/datasets/loader.py`\n",
        "- `ml/datasets/__init__.py` (create if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'touch' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'touch' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'touch' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtouch ml/datasets/__init__.py\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Upload using Colab file upload\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload train_audio.py to ml/training/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# Create __init__.py files (cross-platform)\n",
        "from pathlib import Path\n",
        "Path('ml/__init__.py').touch()\n",
        "Path('ml/training/__init__.py').touch()\n",
        "Path('ml/datasets/__init__.py').touch()\n",
        "print(\"✓ Created __init__.py files\")\n",
        "\n",
        "# Upload using Colab file upload\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"\\nUpload train_audio.py to ml/training/\")\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded:\n",
        "    shutil.move(filename, f'ml/training/{filename}')\n",
        "    print(f\"✓ Moved {filename} to ml/training/\")\n",
        "\n",
        "print(\"\\nUpload logging_config.py to ml/training/\")\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded:\n",
        "    shutil.move(filename, f'ml/training/{filename}')\n",
        "    print(f\"✓ Moved {filename} to ml/training/\")\n",
        "\n",
        "print(\"\\nUpload loader.py to ml/datasets/\")\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded:\n",
        "    shutil.move(filename, f'ml/datasets/{filename}')\n",
        "    print(f\"✓ Moved {filename} to ml/datasets/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Download ASVspoof 2019 Dataset\n",
        "\n",
        "This will download the dataset directly to Colab (faster than local download)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download ASVspoof 2019 LA dataset\n",
        "!mkdir -p ml/datasets/asvspoof_2019\n",
        "!wget -O ml/datasets/asvspoof_2019/LA.zip https://datashare.ed.ac.uk/bitstream/handle/10283/3336/LA.zip\n",
        "\n",
        "# Extract\n",
        "import zipfile\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile('ml/datasets/asvspoof_2019/LA.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('ml/datasets/asvspoof_2019/')\n",
        "print(\"✓ Dataset extracted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Manifest\n",
        "\n",
        "Create a JSONL manifest file for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ml/datasets/build_manifest_simple.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Simple manifest builder for ASVspoof 2019 LA.\"\"\"\n",
        "\n",
        "import json\n",
        "import wave\n",
        "from pathlib import Path\n",
        "\n",
        "def build_manifest(dataset_root, output_path):\n",
        "    dataset_root = Path(dataset_root)\n",
        "    protocol_dir = dataset_root / \"LA\" / \"ASVspoof2019_LA_cm_protocols\"\n",
        "    audio_dir = dataset_root / \"LA\" / \"ASVspoof2019_LA_train\" / \"flac\"\n",
        "    \n",
        "    splits = {\n",
        "        \"train\": protocol_dir / \"ASVspoof2019.LA.cm.train.trn.txt\",\n",
        "        \"dev\": protocol_dir / \"ASVspoof2019.LA.cm.dev.trl.txt\",\n",
        "        \"eval\": protocol_dir / \"ASVspoof2019.LA.cm.eval.trl.txt\",\n",
        "    }\n",
        "    \n",
        "    with open(output_path, 'w') as out:\n",
        "        for split_name, protocol_file in splits.items():\n",
        "            print(f\"Processing {split_name}...\")\n",
        "            if not protocol_file.exists():\n",
        "                print(f\"  Warning: {protocol_file} not found\")\n",
        "                continue\n",
        "            \n",
        "            with open(protocol_file) as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 4:\n",
        "                        continue\n",
        "                    \n",
        "                    speaker_id = parts[0]\n",
        "                    file_id = parts[1]\n",
        "                    label = parts[4] if len(parts) > 4 else \"bonafide\"  # bonafide or spoof\n",
        "                    \n",
        "                    # Find audio file\n",
        "                    audio_path = audio_dir / f\"{file_id}.flac\"\n",
        "                    if not audio_path.exists():\n",
        "                        # Try other split directories\n",
        "                        for try_split in [\"train\", \"dev\", \"eval\"]:\n",
        "                            try_path = dataset_root / \"LA\" / f\"ASVspoof2019_LA_{try_split}\" / \"flac\" / f\"{file_id}.flac\"\n",
        "                            if try_path.exists():\n",
        "                                audio_path = try_path\n",
        "                                break\n",
        "                    \n",
        "                    if not audio_path.exists():\n",
        "                        continue\n",
        "                    \n",
        "                    record = {\n",
        "                        \"path\": str(audio_path),\n",
        "                        \"label\": label,\n",
        "                        \"duration_sec\": 4.0,  # Placeholder\n",
        "                        \"sample_rate\": 16000,\n",
        "                        \"split\": split_name,\n",
        "                    }\n",
        "                    \n",
        "                    out.write(json.dumps(record) + \"\\n\")\n",
        "    \n",
        "    print(f\"✓ Manifest saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_manifest(\n",
        "        \"ml/datasets/asvspoof_2019\",\n",
        "        \"ml/datasets/manifests/asvspoof_2019.jsonl\"\n",
        "    )\n",
        "\n",
        "!mkdir -p ml/datasets/manifests\n",
        "!python ml/datasets/build_manifest_simple.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify Dataset\n",
        "\n",
        "Check that the manifest was created correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "manifest_path = \"ml/datasets/manifests/asvspoof_2019.jsonl\"\n",
        "\n",
        "splits = Counter()\n",
        "labels = Counter()\n",
        "\n",
        "with open(manifest_path) as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        splits[rec['split']] += 1\n",
        "        labels[rec['label']] += 1\n",
        "\n",
        "print(\"Dataset statistics:\")\n",
        "print(f\"  Splits: {dict(splits)}\")\n",
        "print(f\"  Labels: {dict(labels)}\")\n",
        "print(f\"  Total samples: {sum(splits.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Training (Quick Test with Subset)\n",
        "\n",
        "First, let's do a quick test run with a small subset to verify everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test with small subset\n",
        "!python -m ml.training.train_audio \\\n",
        "    --manifest ml/datasets/manifests/asvspoof_2019.jsonl \\\n",
        "    --output-dir models/audio \\\n",
        "    --model-version V0.1.0-test \\\n",
        "    --epochs 2 \\\n",
        "    --batch-size 16 \\\n",
        "    --lr 1e-3 \\\n",
        "    --seed 42 \\\n",
        "    --max-train-samples 500 \\\n",
        "    --max-val-samples 100 \\\n",
        "    --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Full Training Run\n",
        "\n",
        "If the test passed, run full training (this will take 1-3 hours depending on GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training run\n",
        "!python -m ml.training.train_audio \\\n",
        "    --manifest ml/datasets/manifests/asvspoof_2019.jsonl \\\n",
        "    --output-dir models/audio \\\n",
        "    --model-version V1.0.0 \\\n",
        "    --epochs 20 \\\n",
        "    --batch-size 32 \\\n",
        "    --lr 1e-3 \\\n",
        "    --seed 42 \\\n",
        "    --device cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verify ONNX Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Load ONNX model\n",
        "session = ort.InferenceSession(\"models/audio/V1.0.0.onnx\")\n",
        "\n",
        "# Check input/output shapes\n",
        "print(\"Model Inputs:\")\n",
        "for inp in session.get_inputs():\n",
        "    print(f\"  {inp.name}: {inp.shape} ({inp.type})\")\n",
        "\n",
        "print(\"\\nModel Outputs:\")\n",
        "for out in session.get_outputs():\n",
        "    print(f\"  {out.name}: {out.shape} ({out.type})\")\n",
        "\n",
        "# Test inference\n",
        "dummy_input = np.random.randn(1, 64, 1001).astype(np.float32)\n",
        "outputs = session.run(None, {\"audio_features\": dummy_input})\n",
        "print(f\"\\nTest inference output shape: {outputs[0].shape}\")\n",
        "print(f\"Test inference output: {outputs[0]}\")\n",
        "print(\"✓ ONNX model verified\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Download Trained Model\n",
        "\n",
        "Download the ONNX model and metadata to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download ONNX model\n",
        "files.download('models/audio/V1.0.0.onnx')\n",
        "\n",
        "# Download metadata\n",
        "files.download('models/audio/V1.0.0.json')\n",
        "\n",
        "# Download PyTorch checkpoint (optional)\n",
        "files.download('models/audio/V1.0.0_best.pt')\n",
        "\n",
        "print(\"✓ Files ready for download\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Test with Sample Audio (Optional)\n",
        "\n",
        "Test the model with a sample audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load a sample from the dataset\n",
        "sample_path = \"ml/datasets/asvspoof_2019/LA/ASVspoof2019_LA_dev/flac/LA_D_1000147.flac\"  # Adjust path\n",
        "\n",
        "if Path(sample_path).exists():\n",
        "    # Load audio\n",
        "    waveform, sr = torchaudio.load(sample_path)\n",
        "    \n",
        "    # Preprocess (same as training)\n",
        "    from ml.training.train_audio import AudioFeatureExtractor\n",
        "    extractor = AudioFeatureExtractor()\n",
        "    features = extractor(waveform)\n",
        "    features_np = features.unsqueeze(0).numpy()  # Add batch dimension\n",
        "    \n",
        "    # Run inference\n",
        "    outputs = session.run(None, {\"audio_features\": features_np})\n",
        "    logits = outputs[0][0]\n",
        "    probs = np.exp(logits) / np.exp(logits).sum()  # Softmax\n",
        "    \n",
        "    print(f\"Sample: {sample_path}\")\n",
        "    print(f\"Logits: {logits}\")\n",
        "    print(f\"Probabilities: {probs}\")\n",
        "    print(f\"Prediction: {'BONAFIDE' if probs[0] > probs[1] else 'SPOOF'}\")\n",
        "    print(f\"Confidence: {max(probs):.3f}\")\n",
        "else:\n",
        "    print(\"Sample file not found - adjust path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "After running this notebook:\n",
        "1. You'll have a trained ONNX model (`V1.0.0.onnx`)\n",
        "2. Model metadata file (`V1.0.0.json`)\n",
        "3. PyTorch checkpoint for further fine-tuning (`V1.0.0_best.pt`)\n",
        "\n",
        "### Next Steps (on your local machine):\n",
        "```bash\n",
        "# 1. Place the downloaded model\n",
        "cp ~/Downloads/V1.0.0.onnx models/audio/\n",
        "cp ~/Downloads/V1.0.0.json models/audio/\n",
        "\n",
        "# 2. Create symlink\n",
        "cd models/audio\n",
        "ln -s V1.0.0.onnx latest.onnx  # Linux/Mac\n",
        "# Or on Windows: mklink latest.onnx V1.0.0.onnx\n",
        "\n",
        "# 3. Set environment variable\n",
        "export ONNX_MODEL_PATH=models/audio/latest.onnx\n",
        "\n",
        "# 4. Start the API\n",
        "cd services/api\n",
        "uvicorn app.main:app --reload\n",
        "\n",
        "# 5. Test the /v1/models endpoint\n",
        "curl http://localhost:8000/v1/models\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
